# 爬虫配置
crawler:
  concurrency: 300               
  timeout: 20                 
  blacklisted_domains:           # 黑名单配置-共用
    - "cdn.jsdelivr.net"
    - "cdnjs.cloudflare.com"
    - "ajax.googleapis.com"
    - "fonts.googleapis.com"
    - "fonts.gstatic.com"
    - "unpkg.com"
    - "stackpath.bootstrapcdn.com"
    - "facebook.com"
    - "twitter.com"
    - "instagram.com"
    - "linkedin.com"
    - "youtube.com"   


# 数据清洗配置
data_cleaning:
  enable: true                     # 是否启用数据清洗 
  strict_mode: false               # 严格模式：只保留完全匹配的域名
  verbose: false                   # 详细输出被清理的URL
  generate_cleaned_file: false      # 是否生成清洗后的数据文件

# colly 配置
colly:
  enable: true                     # 是否启用 colly
  user_agent:  "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
  timeout: 20                      # 请求超时时间 (秒)
  max_depth: 3                     # 最大爬取深度
  max_pages_per_site: 5000          # 每个站点最多爬取的页面数量
  rate_limit:
    enable: false                  # 是否启用请求速率限制
    qps: 40                        # 每秒请求数限制
  enable_allowed_domains: true     # 是否启用域名限制

# 目录遍历配置
directory_brute:
  enable: true                  # 是否启用目录遍历
  concurrency: 400              # 目录遍历并发数
  timeout: 5                    # 请求超时时间（秒）
  max_dirs_per_site: 0          # 每个站点最大遍历目录数(0表示默认遍历字典全部)（配合爬虫crawl -f 批量扫描建议缩小）
  follow_redirects: false        # 是否跟随重定向
  ignore_403: true              # 是否忽略403状态码
  ignore_404: true              # 是否忽略404状态码
  interesting_status: [200, 201, 202, 301, 302, 403, 500] # 感兴趣的状态码
  with_xss_integration: true #是否开启与XSS扫描集成
  
  # 字典配置
  dictionaries:
    common_dirs: "dicts/common_dirs.txt"  #纯目录遍历功能(适合单独运行遍历漏洞)
    xss_dirs: "dicts/xss_dirs.txt"        #爬虫+遍历(xss 不扫描遍历漏洞)
  
  # 递归遍历配置
  recursive:
    enable: true                  # 是否启用递归遍历
    max_depth: 2                  # 最大递归深度
    only_on_success: true         # 只对成功响应的目录进行递归

katana:
  enable: true                     # 是否启用katana爬虫
  concurrency: 100                  # 并发数 (建议10-20)
  depth: 3                         # 爬取深度 (建议3-5)
  timeout: 20                      # 请求超时时间(秒)
  rate_limit: 500                  # 每秒最大请求数
  include_subs: true               # 是否包含子域名
  js_analysis: true                # 是否启用JS分析 (重要！)
  headless: false                   # 是否使用无头浏览器 (性能考虑)(启用会有leakless.exe进程 火绒可能会误杀 - 这是katana内置的无头浏览器组件)
  form_fill: true                  # 是否自动填充表单 (重要！)
  allow_redirects: false            # 是否允许重定向

# gau 配置
gau:
  enable: true                      # 是否启用 GAU
  include_sub: true                 # 是否包含子域名
  concurrency: 100                   # GAU 独立并发数（建议5-20）
  timeout: 30                       # GAU 独立超时时间 (秒)
  proxy: "http://127.0.0.1:7890"    #(空表示不使用)
  with_spider: true                 # 开启参数差异  /a.php?id=1 和 /a.php?id=2 视为一个(去重)
  mime_filter: true                 # MIME类型过滤 是否只爬取text/html类型的URL(关闭则默认爬全部)
  max_results_per_source: 2000      # 每个数据源最大结果数
  request_delay: 1000               # 请求间延迟(毫秒)
  max_retries: 3                    # 最大重试次数
  providers:                        # 启用的数据源
    - "wayback"
    - "commoncrawl" 
    - "otx"
    - "urlscan"
  blacklist_extensions:             # 黑名单扩展名
    - "png"
    - "jpg" 
    - "gif"
    - "svg"
    - "woff2"
    - "css"
    - "ico"
    - "woff"
    - "ttf"
    - "eot"
  filters:
    min_url_length: 10              # 最短URL长度
    max_url_length: 2000            # 最长URL长度
    exclude_query_params: false     # 是否排除查询参数

# JS分析
js_analysis:
  enable: false            # 是否启用JS分析
  concurrency: 500        # 并发分析

# xss配置
xss:
  enable: true                      # 是否启用XSS扫描
  concurrency: 200                  # 扫描并发数
  timeout: 15                       # 单请求超时时间(秒)
  proxy: ""    # HTTP代理地址 (空表示不使用代理)
  active_scan: true                 # 是否主动发送测试payload
  random_str_length: 8              # 随机标记长度
  min_confidence_score: 0.7         # 最小置信度阈值 (降低以减少误报)
  max_payloads_per_param: 15        # 每参数最大payload数
# 连接池配置(建议默认)
  http_pool:
      max_idle_conns: 500             # 全局最大空闲连接数
      max_idle_conns_per_host: 100    # 每个主机最大空闲连接数  
      max_conns_per_host: 300         # 每个主机最大连接数(设置大于等于并发concurrency以免线程池问题)
      idle_conn_timeout: 90           # 空闲连接超时时间(秒)
      tls_handshake_timeout: 10       # TLS握手超时时间(秒)
      response_header_timeout: 15     # 响应头超时时间(秒)
      disable_keep_alives: false      # 是否禁用keep-alive
      force_attempt_http2: true       # 是否强制尝试HTTP/2

  
  # 参数fuzz配置
  param_fuzzing:
      enable: false                     # 是否启用参数fuzz
      dict_path: "dicts/fuzz.txt"      # fuzz字典路径
      max_params_per_url: 500          # 每个URL最大fuzz参数数量

  ignore_content_types:
    - "image/"                        # 所有图片类型
    - "application/json"              # JSON响应
    - "application/xml"               # XML响应
    - "text/css"                      # CSS文件
    - "text/javascript"               # JavaScript文件
    - "application/javascript"        # JavaScript文件
    - "application/x-javascript"      # JavaScript文件
    - "text/plain"                    # 纯文本文件
    - "application/pdf"               # PDF文件
    - "application/zip"               # 压缩文件
    - "application/octet-stream"      # 二进制文件
    - "font/"                         # 字体文件
    - "audio/"                        # 音频文件
    - "video/"                        # 视频文件

  report:
    enable: true                    # 是否生成报告
    output_dir: "reports"           # 报告输出目录
    template: "xss_report.md"       # 报告模板路径
    real_time: true                 # 启用实时报告
    include_headers: true           # 报告中包含HTTP头信息
    include_raw_request: true       # 包含原始请求
    include_raw_response: false     # 包含原始响应(true=完整响应，false=仅响应头)